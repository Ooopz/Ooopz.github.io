




### 文章目录


+ [一、集成学习介绍](#_2)+ [二、随机森林（Random Forest）](#Random_Forest_17)+ 
+ [1、Bagging](#1Bagging_18)+ [2、随机森林](#2_31)+ [3、sklearn实现RF分类](#3sklearnRF_35)

+ [三、提升方法（Boosting）](#Boosting_61)+ 
+ [1、AdaBoost（Adaptive boosting）](#1AdaBoostAdaptive_boosting_64)+ [2、GBDT（Gradient Boost Decision Tree)](#2GBDTGradient_Boost_Decision_Tree_92)







# 一、集成学习介绍


在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（**弱监督模型，在某些方面表现的比较好**）。**集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型**，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。 集成方法是将几种机器学习技术组合成一个预测模型的算法，以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。

+ 集成学习在各个规模的数据集上都有很好的策略：


+ 数据集大：划分成多个小数据集，学习多个模型进行组合+ 数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合（Bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间）


+ 集合方法可分为两类：


+ 序列集成方法 其中参与训练的基础学习器按照顺序生成（例如 AdaBoost）。序列方法的原理是利用基础学习器之间的依赖关系。通过对之前训练中错误标记的样本赋值较高的权重，可以提高整体的预测效果。+ 并行集成方法 其中参与训练的基础学习器并行生成（例如 Bagging，Random Forest）。并行方法的原理是利用基础学习器之间的独立性，通过平均可以显著降低错误。


# 二、随机森林（Random Forest）


## 1、Bagging


Bagging（bootstrap aggregating，装袋）是并行式集成学习方法代表。

+ 基本思想 初始训练集包含m个样本，**自助采样法采用有放回抽样**，合理性保证至少有近三分之一的初始训练样本不出现在训练样本子集中。通过自助采样法采样出T个含m个样本的训练子集，然后在T个训练子集上训练T个基学习器，在组合这些基学习器，称为Bagging。+ 预测输出


+ 分类——投票法，若遇到同票情形，随机选取或进一步考察学习器投票置信度来决定分类；+ 回归——平均法 
![./figures/20200415221208997.png](./figures/20200415221208997.png)
 **注：Bagging主要关注降低方差；可用于多分类、回归等任务。** ① Bagging通过降低基分类器的方差，改善了泛化误差 ② 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起 ③ 由于**每个样本被选中的概率相同**，因此bagging并不侧重于训练数据集中的任何特定实例


## 2、随机森林


随机森林是Bagging的一个拓展，RF在以**决策树为基学习器构建Bagging集成**的基础上，进一步在决策树的训练过程中引入了**随机属性选择**。在随机森林中，集成中的每棵树都是由从训练集中抽取的样本（即 bootstrap 样本）构建的。另外，与使用所有特征不同，这里随机选择特征子集，从而进一步达到对树的随机化目的。 
![./figures/20200415223130954.png](./figures/20200415223130954.png)
 因此，随机森林产生的偏差略有增加，但是由于**对相关性较小的树计算平均值**，估计方差减小了，导致模型的整体效果更好。

## 3、sklearn实现RF分类


```python
# -*- coding:utf-8 -*-
"""
@author: Tao_RY
@file: RF.py
"""

from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

df = pd.read_csv(r"C:\Users\1\WorkSpace\python\machine learning\data\iris.csv", sep=',')
iris_data = df.iloc[:, 0:3]
iris_target = df.iloc[:, 4]
iris_data_train, iris_data_test, iris_target_train, iris_target_test = train_test_split(iris_data, iris_target,
                                                                                        train_size=.80)
model = RandomForestClassifier(n_estimators=1000)
model.fit(iris_data_train, iris_target_train)   # 创建一个随机森林
y_predict = model.predict(iris_data_test)    # 对新的样本Z做预测
print('accuracy_score:', accuracy_score(iris_target_test, y_predict))
```


# 三、提升方法（Boosting）


其主要思想是将弱分类器组装成一个强分类器。在PAC（probably approximately correct，概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。通过**提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果**。通过加法模型将弱分类器进行线性组合。 
![./figures/20200415233912191.png](./figures/20200415233912191.png)


## 1、AdaBoost（Adaptive boosting）


刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。 
![./figures/20200415235930742.jpg](./figures/20200415235930742.jpg)


+ sklearn实现


```python
# -*- coding:utf-8 -*-
"""
@author: Tao_RY
@file: adaboost.py
"""

import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

df = pd.read_csv(r"C:\Users\1\WorkSpace\python\machine learning\data\iris.csv", sep=',')
iris_data = df.iloc[:, 0:3]
iris_target = df.iloc[:, 4]
iris_data_train, iris_data_test, iris_target_train, iris_target_test = train_test_split(iris_data, iris_target,
                                                                                        train_size=.80)
model = AdaBoostClassifier(n_estimators=1000)
model.fit(iris_data_train, iris_target_train)
y_predict = model.predict(iris_data_test)
print('accuracy_score:', accuracy_score(y_predict, iris_target_test))
```


## 2、GBDT（Gradient Boost Decision Tree)


GBDT（梯度提升迭代决策树）每一次的计算是为了减少上一次的残差，GBDT在残差减少（负梯度）的方向上建立一个新的模型。GBDT 也是 Boosting 算法的一种，但是和 AdaBoost 算法不同（AdaBoost 算法上一篇文章已经介绍）；区别如下：AdaBoost 算法是利用前一轮的弱学习器的误差来更新样本权重值，然后一轮一轮的迭代；GBDT 也是迭代，但是 GBDT 要求弱学习器必须是 CART 模型，而且 GBDT 在模型训练的时候，是要求模型预测的样本损失尽可能的小。 
![./figures/20200416001421185.png](./figures/20200416001421185.png)


+ sklearn实现


```python
# -*- coding:utf-8 -*-
"""
@author: Tao_RY
@file: GBDT.py
"""

import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

df = pd.read_csv(r"C:\Users\1\WorkSpace\python\machine learning\data\iris.csv", sep=',')
iris_data = df.iloc[:, 0:3]
iris_target = df.iloc[:, 4]
iris_data_train, iris_data_test, iris_target_train, iris_target_test = train_test_split(iris_data, iris_target,
                                                                                        train_size=.80)
model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)
model.fit(iris_data_train, iris_target_train)
y_predict = model.predict(iris_data_test)
print('accuracy_score:', accuracy_score(y_predict, iris_target_test))
```


