

**前言：大多数模型都是直接给出公式，其实自己私下有推导，涉及好多自己不懂的数学知识，会一点点补充的**  **机器学习专栏**：[机器学习专栏](https://blog.csdn.net/weixin_43008804/category_9386844.html)




### 文章目录


+ [线性回归（预测）](#_5)+ 
+ [1、单变量线性回归](#1_6)+ 
+ [1.1基本原理](#11_7)+ [1.2最小二乘法](#12_11)+ [1.3sklearn实现单变量线性回归](#13sklearn_34)

+ [2、多元线性回归](#2_88)+ 
+ [2.1基本原理](#21_89)+ [2.2正规方程法](#22_138)+ [2.3梯度下降法](#23_146)+ [2.4sklearn实现多元线性回归](#24sklearn_166)+ [2.5模型优化](#25_215)+ 
+ [2.5.1多项式回归](#251_216)+ [2.5.2sklearn实现多项式回归](#252sklearn_219)











# 线性回归（预测）


## 1、单变量线性回归


### 1.1基本原理


给定数据集$D=(x^{(1)};x^{(2)};...;x^{(m)})$，其中$x^{(i)}$表示第$i$个样本点$x^{(i)}\in{R}$(表示只有一个属性值)，线性回归试图学的预测模型如下： 

$$h_\theta (x^{(i)})=\theta_0+\theta_1x^{(i)}$$

 其中：$h_\theta (x^{(i)} )$表示预测值，$\theta_j$表示模型参数

### 1.2最小二乘法


如何确定$\theta_j$的值呢？——最小二乘法，这是其实一个凸优化问题 均方误差(MSE)：

$$MSE(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}{(h_\theta(x^{(i)})-y^{(i)})^2}$$

 我们通过最小化MSE确定$\theta_j$的值,最小化所有样本到直线上的欧式距离最小，即： 

$$(\theta _0^{*},\theta_1^{*})=\mathop{arg\;min}\limits_{(\theta_0,\theta_1)}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$

 最小二乘法步骤： 1、将$MSE(\theta_0,\theta_1)$对$\theta_0,\theta_1$求导得： 

$$\left\{ \begin{aligned} \frac{\partial MSE(\theta_0,\theta_1)}{\partial \theta_1}=\frac{2}{m}(\theta_1\sum_{i=1}^{m}{x^{(i)}}^2-\sum_{i=1}^{m}(y^{(i)}-\theta_0){x^{(i)}})\\ \frac{\partial MSE(\theta_0,\theta_1)}{\partial \theta_0}=\frac{2}{m}(m\theta_0-\sum_{i=1}^{m}(y^{(i)}-\theta_1{x^{(i)}})) \end{aligned} \right.$$

 2、令上式为0，可以得$\theta_0$和$\theta_1$的闭式解： 

$$\left\{ \begin{aligned} \theta_1=\frac{\sum_{i=1}^{m}y^{(i)}({x^{(i)}} -\bar{x})}{\sum_{i=1}^{m}{x^{(i)}}^2-\frac{1}{m}(\sum_{i=1}^{m}{x^{(i)}})^2}\\ \theta_0=\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-\theta_1{x^{(i)}}) \end{aligned} \right.$$



### 1.3sklearn实现单变量线性回归


这里我使用datasets，熟悉一下，后面将通过pandas加载数据集，更符号实际应用情况

```python
# -*- coding: utf-8 -*-
"""
Created on Sat Nov  9 22:43:52 2019

@author: 1
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

#导入波士顿房价数据集load_diabetes()
diabetes = datasets.load_diabetes()
# Use only one feature
#np.newaxis的作用就是在这一位置增加一个一维
diabetes_X = diabetes.data[:, np.newaxis, 2]
#diabetes_X.shape=(422x1)

#划分训练集和测试集
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]

#构造线性回归模型
regr = linear_model.LinearRegression()
#拟合训练集
regr.fit(diabetes_X_train, diabetes_y_train)
#测试集的预测值
diabetes_y_pred = regr.predict(diabetes_X_test)

#线性模型的系数
print('Coefficients: \n', regr.coef_)
#均方差
print("Mean squared error: %.2f"
      % mean_squared_error(diabetes_y_test, diabetes_y_pred))
#R2决定系数（拟合优度）
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

#真实值
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
#预测值
plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)

#指定坐标轴刻度为无刻度
plt.xticks(())
plt.yticks(())
plt.show()
```


## 2、多元线性回归


### 2.1基本原理


上面介绍的单变量线性回归是比较特殊的情况，通常我们获得的数据是$D={(x^{(1)},y^{(1)});(x^{(2)},y^{(2)});...;(x^{(m)},y^{(i)} )}$，其中$x^{(i)}$表示第$i$个样本点$x^{(i)}\in{R^n}$(表示有n个属性值)，多元线性回归试图学的预测模型如下： 

$$h_\theta(x^{(i)})=\theta_0+\theta_1x_1^{(i)}+\theta_2x_2^{(i)}+....+\theta_nx_n^{(i)}$$

 其中：$h_\theta (x^{(i)} )$表示预测值，$\theta_j$表示模型参数，$\theta_0$亦称为模型偏置（bias）

预测模型可写成向量表达式：$h_\theta(x^{(i)})=\theta^Tx^{(i)}$ 向量表达式参数简单介绍： $\theta=\begin{bmatrix} \theta_0\\\theta_1\\:\\\theta_n \end{bmatrix}$，$x^{(i)}=\begin{bmatrix} x_0^{(i)}\\ x_1^{(i)}\\ :\\ x_n^{(i)} \end{bmatrix}$，其中$x_0^{(i)}$值为1 【注意】多元线性回归与多重线性回归的区别，多重回归是“multiple regerssion”,而多元回归是“multivariate regression”，两者为不同概念，适用的条件不一样，我也不是统计学出身，了解不多，欢迎大家评论区讨论！

多元线性回归的代价函数为： 

$$J(\theta)=\frac {1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$



矩阵形式为： 

$$MSE(\theta)=\frac{1}{m}(X\theta-Y)^T(X\theta-Y) \\ J(\theta)=\frac{1}{2m}(X\theta-Y)^T(X\theta-Y)$$

 其中： $X=\begin{bmatrix} {x^{(1)}}^T\\ {x^{(2)}}^T\\ :\\ {x^{(m)}}^T \end{bmatrix}=\begin{bmatrix} x_0^{(1)} & x_1^{(1)} &... &x_n^{(1)} \\ x_0^{(2)} & x_1^{(2)} &... &x_n^{(2)} \\ : & : &... &:\\ x_0^{(m)} & x_1^{(m)} &... &x_n^{(m)} \end{bmatrix}$表示训练集,$Y=\begin{bmatrix} y^{(1)}\\ y^{(2)}\\ :\\ y^{(m)} \end{bmatrix}$由所有训练样本输出构成的向量，$\theta=\begin{bmatrix} \theta_0\\ \theta_1\\ :\\ \theta_n\end{bmatrix}$

### 2.2正规方程法


同样对于多元线性回归，最小化均方误差（同样这也是一个凸优化问题）： 

$$\theta^*=arg\;minMSE(\theta)$$

 $MSE(\theta)$对$\theta$求导得（涉及矩阵的求导运算）： 

$$\frac{\partial MSE(\theta)}{\partial \theta} =2X^T(X\theta-Y)$$

 令上式为0，可得到$\theta$最优解的闭式解（涉及矩阵的逆运算）： 

$$\theta^*=(X^TX)^{-1}X^TY$$



### 2.3梯度下降法


要理解梯度下降法首先要知道梯度的概念：**梯度的本意是一个向量**，表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）**变化最快**，变化率最大（为该梯度的模），其实这里面的数学知识还是有点复杂的 梯度下降法就是不断找函数下降最快的方向，以找到最优解（线性回归找到的肯定是最优解） 梯度下降公式： 

$$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \\ \theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^{m}((y_\theta(x^{(i)})-y^{(i)})x_j^{(i)})$$

 这里的$\alpha$相当于步长，$\frac{\partial}{\partial\theta_j}J(\theta)$是下降的速率（含方向） **【多元线性回归梯度下降公式的简单推导】** 

$$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \\ \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}[\frac {1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2] \\ \theta_j:=\theta_j-\frac{\alpha}{m}((h_\theta(x^{(i)})-y^{(i)})\frac{\partial h_\theta(x^{(i)})}{\partial \theta_j}) \\ \theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^{m}((y_\theta(x^{(i)})-y^{(i)})x_j^{(i)})$$



### 2.4sklearn实现多元线性回归


```python
# -*- coding: utf-8 -*-
"""
Created on Sat Nov  10 22:43:52 2019

@author: 1
"""

import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
from sklearn.model_selection import train_test_split

#导入波士顿房价数据集load_diabetes()
diabetes_X=pd.read_csv('D:\workspace\python\machine learning\data\diabetes_data.csv\X.csv',sep=' ',header=None)
diabetes_Y=pd.read_csv('D:\workspace\python\machine learning\data\diabetes_target.csv\y.csv',sep=' ',header=None)

#利用sklearn里面的包来对数据集进行划分，以此来创建训练集和测试集
#train_size表示训练集所占总数据集的比例
diabetes_X_train,diabetes_X_test,diabetes_y_train,diabetes_y_test = train_test_split(diabetes_X.iloc[:,:4],diabetes_Y,train_size=.80)
 

#构造线性回归模型
regr = linear_model.LinearRegression()
#拟合训练集
regr.fit(diabetes_X_train, diabetes_y_train)
#测试集的预测值
diabetes_y_pred = regr.predict(diabetes_X_test)

#线性模型的系数
print('Coefficients: \n', regr.coef_)
print('intercept: \n',regr.intercept_)
#均方差
print("Mean squared error: %.2f"
      % mean_squared_error(diabetes_y_test, diabetes_y_pred))
#R2决定系数（拟合优度）
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

#预测值和真实值对比图
plt.plot(range(len(diabetes_y_pred)), diabetes_y_pred, 'b', label="predict")
plt.plot(range(len(diabetes_y_test)), diabetes_y_test, 'r', label="test")
plt.xticks(())
plt.yticks(())
plt.show()
```


### 2.5模型优化


#### 2.5.1多项式回归


当线性模型太简单导致欠拟合的时候，可以通过增加多项式特征来让模型更好的适应我们的数据，比如房价和房子的长以及宽之间的关系等。而且通过观察数据，分析数据的特征，我们可以令$x_1=x_1^2,x_2=x_2^3$将模型转化为线性模型。

#### 2.5.2sklearn实现多项式回归


```python
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 10 19:55:39 2019

@author: 1
"""

#导入多项式模块
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
#管道模型，将线性回归和多项式串起来
from sklearn.pipeline import Pipeline
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

#导入波士顿房价数据集load_diabetes()
diabetes_X=pd.read_csv('D:\workspace\python\machine learning\data\diabetes_data.csv\X.csv',sep=' ',header=None)
diabetes_Y=pd.read_csv('D:\workspace\python\machine learning\data\diabetes_target.csv\y.csv',sep=' ',header=None)

#数据集的划分
diabetes_X_train,diabetes_X_test,diabetes_y_train,diabetes_y_test = train_test_split(diabetes_X,diabetes_Y,train_size=.80)

#线性模型
regr=LinearRegression(normalize=True)#normalize=True表示标准化
pf=PolynomialFeatures(degree=2,interaction_only=True)#interaction_only=True去除高次幂特征
#管道机制实现了对全部步骤的流式化封装和管理
pf_regr=Pipeline([('pf',pf),('regr',regr)])
pf_regr.fit(diabetes_X_train,diabetes_y_train)
diabetes_y_pred = pf_regr.predict(diabetes_X_test)

print('Coefficients: \n',regr.coef_)
print('intercept: \n',regr.intercept_)
#均方差
print("Mean squared error: %.2f" % mean_squared_error(diabetes_y_test, diabetes_y_pred))
#R2决定系数（拟合优度）
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

#预测值和真实值对比图
plt.plot(range(len(diabetes_y_pred)), diabetes_y_pred, 'b', label="predict")
plt.plot(range(len(diabetes_y_test)), diabetes_y_test, 'r', label="test")
plt.xticks(())
plt.yticks(())
plt.show()
```


**回归结果评价**：[回归模型的评价指标](https://blog.csdn.net/chao2016/article/details/84960257)

