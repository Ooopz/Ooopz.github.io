

支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。

## 6.1 间隔与支持向量机


对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：


![./figures/ad6e2c85b5b7535c7b8729612db756d9.png](./figures/ad6e2c85b5b7535c7b8729612db756d9.png)


对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小，对未见示例的泛化能力最强.。因此需要让所选择的超平面能够最大化这个间隔。

样本空间中，划分超平面的的线性方程描述：


![./figures/8a96e67862ce4bae85ade2985df5ac54.png](./figures/8a96e67862ce4bae85ade2985df5ac54.png)


 样本空间中任意点 x 到超平面（
![./figures/gif.latex](./figures/gif.latex)
， b)的距离：


![./figures/dd86182185654006ad7b6fe20392b7ec.png](./figures/dd86182185654006ad7b6fe20392b7ec.png)



![./figures/69910eae0d334c77a62a47b8a19fda41.png](./figures/69910eae0d334c77a62a47b8a19fda41.png)


 如图：距离超平面最近的训练样本使上式等号成立，被称为“**支持向量**”，两个异类支持向量到超平面的距离之和为
![./figures/987e8b0019f4493ea5385a165aca3143.png](./figures/987e8b0019f4493ea5385a165aca3143.png)
，称为**间隔**。


![./figures/8833268e5c394c22b5d534587261558d.png](./figures/8833268e5c394c22b5d534587261558d.png)



![./figures/f6461d1eb9e14c13a3e212551dfa2888.png](./figures/f6461d1eb9e14c13a3e212551dfa2888.png)




于是最大间隔分类器的目标函数定义为：


![./figures/17d41c9a93344599ba95fa0f8702040c.png](./figures/17d41c9a93344599ba95fa0f8702040c.png)



![./figures/2bab987ec048480aab344be22d8b2f47.png](./figures/2bab987ec048480aab344be22d8b2f47.png)


 这就是支持向量机(Support Vector Machine，简称 SVM)的基本型.

## 6.2 对偶问题


对于上述得到的目标函数，为一个**带约束的凸二次规划问题**，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的**对偶问题**，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：

```bash
* 一是因为使用对偶问题更容易求解；  
* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。  ```


对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。


![./figures/5d1c53297c724011b9d0547a29b18054.png](./figures/5d1c53297c724011b9d0547a29b18054.png)


**SMO**算法求解：


![./figures/9e320c087b834970a95c296e1d6f0f81.png](./figures/9e320c087b834970a95c296e1d6f0f81.png)


+ 选取一对需更新的变量 
![./figures/1306a5f1c2fd43f08fbeedc2cc6d1947.png](./figures/1306a5f1c2fd43f08fbeedc2cc6d1947.png)
+ 固定αi以外的参数，求解式(6.11)获得更新后的αi 和 αj


KKT 条件违背的程度越大 ，则变量更新后可能导致的目标函数值减幅越大，SMO 先选取违背 KKT 条件程度最大的变量。第二个变量应选择一个使目标函数值减小最快的变量，由于比较各变量所对应的目标函数值减幅的复杂度过高，因此 SMO 采用了一 个启发式：使选取的两变量所对应样本之间的问隔最大.

SMO 算法之所以高效，恰由于在固定其他参数后，仅优化两个参数的过程 能做到非常高效。


![./figures/873564ecdd2943e1a6fa1f0af4ec0880.png](./figures/873564ecdd2943e1a6fa1f0af4ec0880.png)


## **6.3 核函数**


由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用**映射**的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：


![./figures/32dde35a9034847b11180775e4356e3e.png](./figures/32dde35a9034847b11180775e4356e3e.png)


按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：

（1）原对偶问题变为：


![./figures/2b7fa0ba879d4e51b8dc300cc1783874.png](./figures/2b7fa0ba879d4e51b8dc300cc1783874.png)



![./figures/19102ecd76184fd5a3a8c44f44c3e08b.png](./figures/19102ecd76184fd5a3a8c44f44c3e08b.png)


 （2）直接计算  
![./figures/dc2c985ce5034f60aa37ebebc4952ad7.png](./figures/dc2c985ce5034f60aa37ebebc4952ad7.png)
，求解的过程中，只涉及到了高维特征空间中的内积运算，通常是困难的。设想函数（核函数）：


![./figures/672f182d665d4563bd71429b09adb11c.png](./figures/672f182d665d4563bd71429b09adb11c.png)



![./figures/851fe5284c3d52904858c15ce3b7bc60.png](./figures/851fe5284c3d52904858c15ce3b7bc60.png)


因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效**（低维计算，高维表现）**，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：

（1）对偶问题：


![./figures/cc3b7a13d386e9e3fc5a4a6eda03cfcc.png](./figures/cc3b7a13d386e9e3fc5a4a6eda03cfcc.png)


（2）求解后得到分类函数：


![./figures/d181fc2e06379ffa1f15f149208877a1.png](./figures/d181fc2e06379ffa1f15f149208877a1.png)


因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：


![./figures/3bb057c163533971bc87a4a12ff94bea.png](./figures/3bb057c163533971bc87a4a12ff94bea.png)


只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。，任何一个核函数都隐式地定义了一个称为"**再生核希尔伯特空间**" (Reproducing Kernel Hilbert Space，简称 RKHS)的特征空间。

由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：


![./figures/69c35f33ba64b589ee974db50019c41b.png](./figures/69c35f33ba64b589ee974db50019c41b.png)


若 κ1 和 κ2 为核函数，则对于任意正数 
![./figures/958dd065a72f40d3a1698a222419bf29.png](./figures/958dd065a72f40d3a1698a222419bf29.png)
， 

线性组合：


![./figures/e2f37d2a0fdb4026b63931424f7d062c.png](./figures/e2f37d2a0fdb4026b63931424f7d062c.png)


核函数直积：


![./figures/6b20a7c225934a1b94f2b1eee0ecc6ee.png](./figures/6b20a7c225934a1b94f2b1eee0ecc6ee.png)


任意函数g(x)：


![./figures/916750aa89da48baaa8dfe2916f2798a.png](./figures/916750aa89da48baaa8dfe2916f2798a.png)


** 都是核函数**

## 6.4 软间隔支持向量机


前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有**噪声**的情形，噪声数据（**outlier**）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。


![./figures/7612b7a44d0c026968cf4e40f207d679.png](./figures/7612b7a44d0c026968cf4e40f207d679.png)


为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了**“软间隔”支持向量机**的概念。

允许某些样本不满足约束： 


![./figures/36b1dd6666b74c0cbb2dee903444c724.png](./figures/36b1dd6666b74c0cbb2dee903444c724.png)


在最大化间隔的同时，不满足约束的样本应尽可能少。

这样优化目标变为：


![./figures/6282dbf2aff246358057e9b1327f9c45.png](./figures/6282dbf2aff246358057e9b1327f9c45.png)


C无穷大，是所有样本满足约束，C取有限值，允许一些样本不满足约束。


![./figures/9c90ee61608a49468b849dbfefcd39cb.png](./figures/9c90ee61608a49468b849dbfefcd39cb.png)
损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。


![./figures/3e113a089e0c14e0a187bbdc16325d5d.png](./figures/3e113a089e0c14e0a187bbdc16325d5d.png)



![./figures/6088c8e614f94c3f83a289f268d481d0.png](./figures/6088c8e614f94c3f83a289f268d481d0.png)


 支持向量机中的损失函数为**hinge损失**，引入**“松弛变量”**，目标函数与约束条件可以写为：


![./figures/45eddfac1a196480300832a534a56d9e.png](./figures/45eddfac1a196480300832a534a56d9e.png)


其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：


![./figures/fb96b01692c35d90256e01c6e2d57a02.png](./figures/fb96b01692c35d90256e01c6e2d57a02.png)


按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：


![./figures/99dfbe3d97ecc6785d0b9862d7746167.png](./figures/99dfbe3d97ecc6785d0b9862d7746167.png)


将w代入L化简，便得到其对偶问题：


![./figures/f7c12f46e8da4e508180e79eb38e556f.png](./figures/f7c12f46e8da4e508180e79eb38e556f.png)


将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。


![./figures/cc2c99b1ee684a5a84c20d399649cf12.png](./figures/cc2c99b1ee684a5a84c20d399649cf12.png)


 能否使用其他的替代损失函数？？？

如果使用对率损失函数
![./figures/700943d8540443ecb0901ab82f81fef2.png](./figures/700943d8540443ecb0901ab82f81fef2.png)
则儿乎就得到了对率回归模型。实际上，支持向量机与对率回归的优化 目标相近。区别就是前者输出不具有概率意义。hinge 损失有一块"平坦"的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似支持向量的概念，因此对率回归的解依赖于更多的训练样本，其预测开销更大。

0/1 损失函数换成别的替代损失函数以得到 其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一 个共性:

优化目标中的第一项用来描述划分超平面的"间隔"大小，另一项
![./figures/52850ba7be5545c286833c4c9f380ca1.png](./figures/52850ba7be5545c286833c4c9f380ca1.png)
用来表述训练集上的误差。一般形式：


![./figures/5579f874b2474336ae5917aed541ea91.png](./figures/5579f874b2474336ae5917aed541ea91.png)


"结构风险"（**正则化项**） ：描述模型 f 的某些性质。表述了我们希望获得具有何种性质的模型。

"经验风险"：于描述模型与训练 数据的契合程度。C（**正则化常数**） 用于对二者进行折中

## 6.5 支持向量回归


考虑回归问题，给定样本希望f(x)与y尽可能接近，计算损失，当两者相等时损失才为零。与此不同的是支持向量回归（Support Vector Regression，简称 SVR），假设能容忍 f(x) 与 y 之间最多有 
![./figures/gif.latex](./figures/gif.latex)
，即当f(x) 与 y 差别的绝对值大于
![./figures/gif.latex](./figures/gif.latex)
时才计算损失。如图相当于以 f(x) 为中心，构建了一个2
![./figures/gif.latex](./figures/gif.latex)
的间隔带，间隔带中的样本将被预测为正确的。


![./figures/ff2d63f8bd2a44c290de2725c5c5463d.png](./figures/ff2d63f8bd2a44c290de2725c5c5463d.png)


 SVR问题形式化为：


![./figures/a9cb284ab6264a81a1ba693840e20e62.png](./figures/a9cb284ab6264a81a1ba693840e20e62.png)


 其中 C 为正则化常数，
![./figures/47a319217454417ab1c7e2ea23394c0c.png](./figures/47a319217454417ab1c7e2ea23394c0c.png)
是
![./figures/gif.latex](./figures/gif.latex)
-不敏感损失函数：


![./figures/36a462ee91a442618f8236b43b364fc6.png](./figures/36a462ee91a442618f8236b43b364fc6.png)



![./figures/2d2f9c328e4c43318f07608e62e3ceaa.png](./figures/2d2f9c328e4c43318f07608e62e3ceaa.png)



![./figures/3426e153c01f470c8fb1ccd835850418.png](./figures/3426e153c01f470c8fb1ccd835850418.png)


 求解方法与类似，得出分类函数：


![./figures/68429b3e972447ea83fd7a20c878e02b.png](./figures/68429b3e972447ea83fd7a20c878e02b.png)



![./figures/4dd2aacae26f4b1eb073beeacad4e67e.png](./figures/4dd2aacae26f4b1eb073beeacad4e67e.png)
 的样本即为SVR的支持向量，必须在间隔带之外。

## 6.6 核方法


若不考虑偏移项，无论 SVM 还是 SVR，学得的模型总能表 示成核函数
![./figures/d446bcea0fe44543a08f13e4a728ca71.png](./figures/d446bcea0fe44543a08f13e4a728ca71.png)
 的线性组合。


![./figures/2be71748cbc343c388331d7daddabbe6.png](./figures/2be71748cbc343c388331d7daddabbe6.png)


表示定理对损失函数没有限制，对正则化项
![./figures/5b8feda3f17a42118a6f2ae920ea0976.png](./figures/5b8feda3f17a42118a6f2ae920ea0976.png)
 仅要求单调递增，甚至不要求 
![./figures/29f9815b8ae943179bd1a0b6f0b86a8b.png](./figures/29f9815b8ae943179bd1a0b6f0b86a8b.png)
 是凸函数，意味着对于一般的损失函数和正则化项，优化问题的最优解
![./figures/f5da48d4eb1b43b9b50fd7373a0e3de2.png](./figures/f5da48d4eb1b43b9b50fd7373a0e3de2.png)
都可表示为核函数的线性组合。

 一系列基于核函数的学习方法，统称为"**核方**法" (kernel methods)，最常见的，是通过"核化"来将线性学习器拓展为非线性学习器。

例：线性判别分析核化非线性扩展得到**核线性判别分析**(Kernelized Linear Discriminant Analysis，简称 KLDA)，具体过程不详述。参考课本p137-139

