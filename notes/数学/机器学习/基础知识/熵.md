# 熵

## 信息熵

### 信息量

信息熵反应的是一个信息的不确定度，在一个随机事件中，某个事件发生的不确定度越大，熵也就越大，那我们要搞清楚所需要的信息量越大

假设两个随机变量 $x$ 和 $y$ 是相互独立的，那么分别观测两个变量得到的信息量应该和同时观测两个变量的信息量是相同的，我们用 $h$ 来表示信息量，那信息量应该满足：

$$h(x+y) = h(x) + h(y)$$

对于 $x$ 和 $y$ 两个独立事件，他们分别发生的概率是 $p(x)$ 和 $p(y)$，那么 $p(x)$ 和 $p(y)$ 同时发生的概率是：

$$p(x,y) = p(x)p(y)$$

因此可以在事件 $x$ 和 $y$ 中获得的预期信息量可以通过求二者的期望得到：

$$H(x) = p(x)h(x) + p(y)h(y)$$

我们现在回到信息量 $h$ 中， 在我们生活中，信息量的大小跟随机事件的概率有关。越小概率的事情发生所产生的信息量越大，越大概率的事情发生了产生的信息量越小，也就是说：

1.  当一个事件发生的概率 $p(x)$ 为 $1$ 并且它发生了，那我们等到的信息量是 $h(x) = 0$
2.  当一个事件发生的概率 $p(x)$ 为 $0$ 并且它发生了，那我们得到的信息可能是无限大
3.  $H(x)$ 随 $p(x)$ 单调递增。
4.  $p(x,y) = p(x)p(y)$
5.  $h(x,y) = h(x) + h(y)$
6.  信息量 $h(x)$ 反比于 $p(x)$
7.  信息量是非负的

因此可以构建表达式描述信息量与概率的关系：

$$h(x) = - \log_{2}(p(x))$$

对于 log 的底数，现在主流的认识是 log 以 2 为底，我们一般考虑到一个事件的信息量是一连串相互独立随机变量发生的结果，其中每一个选择都在 0 或 1 之间做出，我们能算出所有可能结果数为 $N=2^n$，$n$ 是独立随机变量的个数， 于是，为了将指数形式变成线性形式就是 $n = log_{2}(N)$ 了。

### 信息熵

信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。因此香农提出了对信息熵的定义：**无损编码事件信息的最小平均编码长度**

有了描述信息量的公式后，量化一系列事件的信息熵公式就显而易见了：

$$H(x) = - \sum_{x \in\{X, Y, Z, W \ldots\}} p(x) \log_{2}(p(x))$$

显然的，对于离散变量 $x$ 的概率分布 $P(x)$，熵的公式为：

$$H(x)=-\sum_{x} P(x) {\log _{2} P(x)}$$

对于连续变量 $x$ 的概率分布 $P(x)$，熵的公式为：

$$H(x)=-\int P(x) \log _{2} P(x) d x$$

## 交叉熵

在信息熵的公式中，对于离散变量和连续变量，我们都是计算了*负的可能性的对数的期望*，代表了该事件理论上的**平均最小编码长度**。

而交叉熵，是用来衡量在给定的真实概率分布 $p(x)$ ，使用非真实概率分布 $q(x)$（抽样得到的或拟合出的）来描述真实概率分布所用的**平均最小编码长度**。

公式如下：

$$
H(p,q)=-\sum_{x} p(x) {\log _{2} q(x)}
$$


## 相对熵

相对熵又称 **KL 散度**，用于衡量对于同一个随机变量 $x$ 的两个分布 $p(x)$ 和 $q(x)$ **差异**的**非对称**度量。

提到差异，我们自然能够想到用两个分布的熵的差值作为度量标准，同样 $p(x)$ 可以看作是 $x$ 的真实分布概率，$q(x)$ 可以看作是 $x$ 的观测到（或拟合出）的分布概率。事实上，相对熵就是交叉熵减去熵。

公式如下：

$$
\begin{aligned}
D_{K L}(p \| q) &= H(p,q)-H(p)\\
&= -\sum_{x} p(x) {\log _{2} q(x)} - \left(-\sum_{x} p(x) {\log _{2} p(x)}\right) \\
&=\sum_{x} p\left(x\right) \log_{2} \left(\frac{p\left(x\right)}{q\left(x\right)}\right)
\end{aligned}
$$


