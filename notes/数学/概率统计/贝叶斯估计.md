# 贝叶斯估计

在概率论中有两大学派，[频率学派](频率学派)和[贝叶斯学派](贝叶斯学派)。

频率学派认为随机变量服从特定的统计分布规律，分布函数的参数是确定的数，可以通过抽样来估计。

和频率学派不同，贝叶斯学派认为一切皆为随机变量，随机变量的分布函数的参数也是随机变量，对其进行抽样估计时还必须考虑参数的先验分布。

贝叶斯估计是典型的贝叶斯学派观点，它的基本思想是：待估计参数 $\boldsymbol \theta$ 也是随机的，和一般随机变量没有本质区别，因此只能根据观测样本估计参数 $\boldsymbol \theta$ 的分布。

贝叶斯估计利用了贝叶斯公式，下面给出贝叶斯估计的数学描述：

离散变量：

$$P(B_i|A)=\frac {P(B_i)P(A|B_i)}{P(A)}=\frac {P(B_i)P(A|B_i)}{\sum^{n}_{j=1} P(B_j)P(A|B_j)}$$

连续变量：

$$
\pi (\boldsymbol \theta|\boldsymbol x)=\frac {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}{m(\boldsymbol x)}=\frac {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}{\int f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)d(\boldsymbol \theta)}$$
$$\boldsymbol {\hat \theta}=E\pi (\boldsymbol \theta|\boldsymbol x)
$$

其中， $\pi (\boldsymbol \theta)$ 为参数 $\boldsymbol \theta$ 的先验分布（prior distribution），表示对参数 $\boldsymbol \theta$ 的主观认识，是非样本信息， $\pi (\boldsymbol \theta|\boldsymbol x)$ 为参数 $\boldsymbol \theta$ 的后验分布（posterior distribution）。因此，贝叶斯估计可以看作是，在假定 $\boldsymbol \theta$ 服从 $\pi (\boldsymbol \theta)$ 的先验分布前提下，根据样本信息去校正先验分布，得到后验分布 $\pi (\boldsymbol \theta|\boldsymbol x)$ 。*由于后验分布是一个条件分布，通常我们取后验分布的期望作为参数的估计值。*

## 1. 最大后验估计

贝叶斯估计也叫做最大后验概率估计法，简称 **MAP(Maximum A Posterior)**。

可以认为[极大似然估计](极大似然估计)是贝叶斯估计*不考虑先验概率的特例*。

在贝叶斯学派中，似然函数被理解为 $x_1,x_2,x_3,..,x_N$ 在 $\pmb{\theta}$ 已知时的条件概率：

$$\pi(x_1,x_2,x_3,...,x_N\,|\,\pmb{\theta}) = L(x_1,x_2,x_3,...,x_N\,|\,\pmb{\theta}) = \prod_\limits{i=1}^{N} \pi(x_i\,|\,\pmb{\theta})$$

而 $\pmb{\theta}$ 本身也为随机变量，具有先验概率分布函数 $\pi(\pmb{\theta})$ 。

贝叶斯估计的想法是最大化 $\pmb{\theta}$ 的后验概率，应用贝叶斯公式得到：

$$\boldsymbol {\hat \theta}_{map}=arg \underset {\boldsymbol \theta}{\max} \pi (\boldsymbol \theta|\boldsymbol x)=arg \underset {\boldsymbol \theta}{\max} \frac {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}{m(\boldsymbol x)}=arg \underset {\boldsymbol \theta}{\max}  {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}$$

由于 $m(\boldsymbol x)$ 与 $\boldsymbol \theta$ 无关，因此简化了计算。

当不考虑先验概率 $\pi(\pmb{\theta})$ 时，最大化后验概率等同于极大似然估计。

由于在实践中，先验概率 $\pi(\pmb{\theta})$ 往往并不可知，所以极大似然估计法用的更多一些。

在机器学习中，有一种和引入先验概率等效的做法，那就是在目标函数(相当于对数似然函数)后面加入正则化项。

$$\hat \theta_{map} = arg \underset {\boldsymbol \theta}{\max}  {f(\boldsymbol x|\boldsymbol \theta)\pi (\boldsymbol \theta)}=arg \underset {\boldsymbol \theta}{\max}  (\log {f(\boldsymbol x|\boldsymbol \theta)+ \log \pi (\boldsymbol \theta))}$$

如果将机器学习结构风险中的正则化项对应为上式的 $\log \pi (\boldsymbol \theta)$ ，那么带有正则化项的最大似然学习就可以被解释为 MAP。当然，这并不是总是正确的，例如，有些正则化项可能不是一个概率分布的对数，还有些正则化项依赖于数据，当然也不会是一个先验概率分布。不过，MAP 提供了一个直观的方法来设计复杂但可解释的正则化项，例如，更复杂的惩罚项可以通过混合高斯分布作为先验得到，而不是一个单独的高斯分布。

如果加入的是 [L1 正则化](L1%20正则化)，相当于假设了参数的先验分布符合双指数分布，而如果引入了 [L2 正则化](L2%20正则化)，相当于假设了参数的先验分布符合正态分布。

在机器学习中，[经验风险](经验风险)最小化和极大似然估计对应，[结构风险](结构风险)最小化和贝叶斯估计对应。

## 2. 共轭先验

在贝叶斯估计中，如果选取先验分布 $\pi (\boldsymbol \theta)$ ，使得后验分布 $\pi (\boldsymbol \theta|\boldsymbol x)$ 与 $\pi (\boldsymbol \theta)$ 属于同一分布簇（即共轭分布），则称 $\pi (\boldsymbol \theta)$ 为似然函数 $f(\boldsymbol x|\boldsymbol \theta)$ 的共轭先验。

共轭先验的选取有如下好处：
1.  符合直观，先验分布和后验分布应该是相同形式的；
2.  可以给出后验分布的解析形式；
3.  可以形成一个先验链，即现在的后验分布可以作为下一次计算的先验分布，如果形式相同，就可以形成一个链条。

常见的共轭先验有：Beta 分布（二项分布）、Dirichlet 分布（多项分布）。

很显然，共轭先验的选取很大程度上是基于数学理论的方便性，带有很强的主观色彩，而这也是饱受频率学派诟病的一点。频率学派认为，只有在先验分布有一种不依赖主观的意义，且能根据适当的理论或以往的经验决定时，才允许在统计推断中使用先验分布，否则就会丧失客观性。
