# 参数估计

## 1. 点估计

设总体 $X$ 的分布函数形式已知，但他的一个或多个参数未知，借助于总体 $X$ 的一个样本来估计总体未知参数的值的问题称为参数的点估计问题。

点估计问题的一般提法如下：

设总体 $X$ 的分布函数 $F(x ; \theta)$ 的形式为已 知, $\theta$ 是待估参数。 $X_{1}, X_{2}, \cdots, X_{n}$ 是 $X$ 的一个样本, $x_{1}, x_{2}, \cdots, x_{n}$ 是相应的一个 样本值。点估计问题就是要构造一个适当的统计量 $\hat{\theta}\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ , 用它的观 察值 $\hat{\theta}\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ 作为末知参数 $\theta$ 的近似值. 我们称 $\hat{\theta}\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 为 $\theta$ 的**估计量**，称 $\hat{\theta}\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ 为 $\theta$ 的**估计值**。在不致混淆的情况下统称估计量和估计值为估计，并都简记为$\hat{\theta}$。由于估计量是样本的函数，因此对于不同的样本值， $\theta$ 的估计值一般是不相同的。

### 1.1 矩估计

矩估计的核心思想是*用样本矩来代替总体矩*，对于一个待估总体来说，如果有 $k$ 个参数待估，就通过求样本的从 $1$ 到 $k$ 阶矩得到 $k$ 个方程，联立方程后可以求得 $k$ 个待估参数。

具体如下：

设 $X$ 为连续型随机变量，其概率密度为 $f\left(x ; \theta_{1}, \theta_{2}, \cdots, \theta_{k}\right)$，或 $X$ 为离散型随机变量，其分布律为 $P\{X=x\}=p\left(x ; \theta_{1}, \theta_{2}, \cdots, \theta_{k}\right)$，其中 $\theta_{1}, \theta_{2}, \cdots, \theta_{k}$ 为待估参数，$X_{1}, X_{2}, \cdots, X_{n}$ 是来自 $X$ 的样本。 

假设总体 $X$ 的前 $k$ 阶矩：
$$\mu_{l}=E\left(X^{l}\right)=\int_{-\infty}^{\infty} x^{l} f\left(x ; \theta_{1}, \theta_{2}, \cdots, \theta_{k}\right) \mathrm{d} x \quad(X \text { 连续型）}$$

或

$$\mu_{l}=E\left(X^{l}\right)=\sum_{x \in R_{X}} x^{l} p\left(x ; \theta_{1}, \theta_{2}, \cdots, \theta_{k}\right) \quad(X \text { 离散型） }$$
存在。 其中$R_{X}$ 是 $X$ 可能取值的范围。一般来说，它们是$\theta_{1}, \theta_{2}, \cdots, \theta_{k}$ 的函数。

基于样本矩

$$A_{l}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{l}$$

依概率收敛于相应的总体矩 $\mu_{l}(l=1,2, \cdots, k)$，样本矩的连续函数依概率收敛于相应的总体矩的连续函数，我们就*用样本矩作为相应的总体矩的估计量*，同时*以样本矩的连续函数作为相应的总体矩的连续函数的估计量*。这种估计方法称为**矩估计法**。

矩估计法的具体做法如下，设：

$$
\left\{\begin{aligned}
\mu_{1} &=\mu_{1}\left(\theta_{1}, \theta_{2}, \cdots, \theta_{k}\right), \\
\mu_{2} &=\mu_{2}\left(\theta_{1}, \theta_{2}, \cdots, \theta_{k}\right), \\
& \vdots \\
\mu_{k} &=\mu_{k}\left(\theta_{1}, \theta_{2}, \cdots, \theta_{k}\right) .
\end{aligned}\right.
$$

这是一个包含 $k$ 个末知参数 $\theta_{1}, \theta_{2}, \cdots, \theta_{k}$ 的联立方程组，一般来说，可以从中解 出 $\theta_{1}, \theta_{2}, \cdots, \theta_{k}$，得到：

$$
\left\{\begin{array}{c}
\theta_{1}=\theta_{1}\left(\mu_{1}, \mu_{2}, \cdots, \mu_{k}\right), \\
\theta_{2}=\theta_{2}\left(\mu_{1}, \mu_{2}, \cdots, \mu_{k}\right), \\
\quad \vdots \\
\theta_{k}=\theta_{k}\left(\mu_{1}, \mu_{2}, \cdots, \mu_{k}\right) .
\end{array}\right.
$$

以 $A_{i}$ 分别代替上式中的 $\mu_{i}, i=1,2, \cdots, k$，就以

$$\hat{\theta}_{i}=\theta_{i}\left(A_{1}, A_{2}, \cdots, A_{k}\right), i=1,2, \cdots, k$$

分别作为 $\theta_{i}, i=1,2, \cdots, k$ 的估计量，这种估计量称为**矩估计量**。矩估计量的观察值称为**矩估计值**。

### 1.2 极大似然估计

#### 1.2.1 离散型

若总体 $X$ 属离散型，其分布律 $P\{X=x\}=p(x ; \theta), \theta \in \Theta$ 的形式为已知，$\theta$ 为待估参数, $\Theta$ 是 $\theta$ 可能取值的范围。设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自 $X$ 的样本，则 $X_{1} , X_{2}, \cdots, X_{n}$ 的联合分布律为：

$$\prod_{i=1}^{n} p\left(x_{i} ; \theta\right)$$

又设 $x_{1}, x_{2}, \cdots, x_{n}$ 是相应于样本 $X_{1}, X_{2}, \cdots, X_{n}$ 的一个样本值。易知样本 $X_{1} , X_{2}, \cdots, X_{n}$ 取到观察值 $x_{1}, x_{2}, \cdots, x_{n}$ 的概率，亦即事件 $\left\{X_{1}=x_{1}, X_{2}=x_{2}, \cdots, X_{n}\right. \left.=x_{n}\right\}$ 发生的概率为：

$$L(\theta)=L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta\right)=\prod_{i=1}^{n} p\left(x_{i} ; \theta\right), \theta \in \Theta$$

这一概率随 $\theta$ 的取值而变化, 它是 $\theta$ 的函数, $L(\theta)$ 称为样本的似然函数（注意，这 里 $x_{1}, x_{2}, \cdots, x_{n}$ 是已知的样本值，它们都是常数）。

关于最大似然估计法，我们有以下的直观想法：

现在已经取到样本值 $x_{1} , x_{2}, \cdots, x_{n}$ 了，这表明取到这一样本值的概率 $L(\theta)$ 比较大。

我们当然不会考虑那些不能使样本 $x_{1}, x_{2}, \cdots, x_{n}$ 出现的 $\theta \in \Theta$ 作为 $\theta$ 的估计，再者，如果已知当 $\theta=\theta_{0} \in \Theta$ 时使 $L(\theta)$ 取很大值，而 $\Theta$ 中的其他 $\theta$ 的值使 $L(\theta)$ 取很小值，我们自然认为取 $\theta_{0}$ 作为末知参数 $\theta$ 的估计值较为合理。

由费希尔 (R. A. Fisher) 引进的最大似然估计法，就是固定样本观察值 $x_{1}, x_{2}, \cdots, x_{n}$ ，在 $\theta$ 取值的可能范围 $\Theta$ 内挑选，使似然函数 $L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta\right)$ 达到最大的参数值 $\hat{\theta}$，作为参数 $\theta$ 的估计值。

即取 $\hat{\theta}$ 使：

$$L\left(x_{1}, x_{2}, \cdots, x_{n} ; \hat{\theta}\right)=\max _{\theta \in \Theta} L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta\right)$$

这样得到的 $\hat{\theta}$ 与样本值 $x_{1}, x_{2}, \cdots, x_{n}$ 有关，常记为 $\hat{\theta}\left(x_{1}, x_{2}, \cdots, x_{n}\right)$，称为参数 $\theta$ 的最大似然估计值，而相应的统计量 $\theta\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 称为参数 $\theta$ 的最大似然 估计量。

#### 1.2.2 连续型

若总体 $X$ 属连续型, 其概率密度 $f(x ; \theta), \theta \in \Theta$ 的形式已知，$\theta$ 为待估参数, $\Theta$ 是 $\theta$ 可能取值的范围。设 $X_{1}, X_{2}, \cdots, X_{n}$ 是来自 $X$ 的样本，则 $X_{1}, X_{2}, \cdots, X_{n}$ 的 联合密度为：

$$\prod_{i=1}^{n} f\left(x_{i}, \theta\right)$$

设 $x_{1}, x_{2}, \cdots, x_{n}$ 是相应于样本 $X_{1}, X_{2}, \cdots, X_{n}$ 的一个样本值，则随机点 $( X_{1} , X_{2}, \cdots, X_{n} )$ 落在点 $\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ 的邻域（边长分别为 $\mathrm{d} x_{1}, \mathrm{~d} x_{2}, \cdots, \mathrm{d} x_{n}$ 的 $n$ 维立方体）内的概率近似地为：

$$\prod_{i=1}^{n} f\left(x_{i} ; \theta\right) \mathrm{d} x_{i}$$

其值随 $\theta$ 的取值而变化。

与离散型的情况一样，我们取 $\theta$ 的估计值 $\hat{\theta}$ 使概率 $\prod_{i=1}^{n} f\left(x_{i} ; \theta\right) \mathrm{d} x_{i}$ 取到最大值，但因子 $\prod_{i=1}^{n} \mathrm{~d} x_{i}$ 不随 $\theta$ 而变, 故只需考虑函数

$$L(\theta)=L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta\right)=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right)$$

的最大值。这里 $L(\theta)$ 称为样本的似然函数。

若：

$$L\left(x_{1}, x_{2}, \cdots, x_{n} ; \hat{\theta}\right)=\max _{\theta \in \theta} L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta\right)$$

则称 $\hat{\theta}\left(x_{1}, x_{2}, \cdots, x_{n}\right)$ 为 $\theta$ 的**最大似然估计值**, 称 $\hat{\theta}\left(X_{1}, X_{2}, \cdots, X_{n}\right)$ 为 $\theta$ 的**最大似然估计量**。

这样，*确定最大似然估计量的问题就归结为微分学中的求最大值的问题了*。

在很多情形下， $p(x ; \theta)$ 和 $f(x ; \theta$) 关于 $\theta$ 可微, 这时 $\hat{\theta}$ 常可从方程

$$\frac{\mathrm{d}}{\mathrm{d} \theta} L(\theta)=0$$

解得 $\Phi$ 。又因 $L(\theta)$ 与 $\ln L(\theta)$ 在同一 $\theta$ 处取到极值，因此，$\theta$ 的最大似然估计 $\theta$ 也可 以从方程

$$\frac{\mathrm{d}}{\mathrm{d} \theta} \ln L(\theta)=0$$

求得，而从后一方程求解往往比较方便。$\frac{\mathrm{d}}{\mathrm{d} \theta} \ln L(\theta)=0$称为**对数似然方程**。

最大似然估计法也适用于分布中含多个末知参数 $\theta_{1}, \theta_{2}, \cdots, \theta_{k}$ 的情况。这时，似然函数 L 是这些末知参数的函数。分别令

$$\frac{\partial}{\partial \theta_{i}} L=0, i=1,2, \cdots, k$$

或令 

$$\quad \frac{\partial}{\partial \theta_{i}} \ln L=0, i=1,2, \cdots, k$$


上述方程称为**最大似然方程组**，解上述由 $k$ 个方程组成的方程组，即可得到各末知参数 $\theta_{i}(i=1,2, \cdots, k)$ 的最大似然估计值 $\hat{\theta}$ 。

此外。最大似然估计具有下述性质：设 $\theta$ 的函数 $u=u(\theta), \theta \in \Theta$ 具有[单值反函数](单值反函数) $\theta=\theta(u), u \in \mathbb{U}$ 又假设 $\dot{\theta}$ 是 $X$ 的概率分布中参数 $\theta$ 的最大似然估计， 则 $\hat{u}=u(\hat{\theta})$ 是 $u(\theta)$ 的最大似然估计。 这一性质称为最大似然估计的不变性。

事实上，因为 $\hat{\theta}$ 是 $\theta$ 的最大似然估计，于是有：

$$L\left(x_{1}, x_{2}, \cdots, x_{n} ; \hat{\theta}\right)=\max _{\theta \in \theta} L\left(x_{1}, x_{2}, \cdots, x_{n}, \theta\right)$$

其中 $x_{1}, x_{2}, \cdots, x_{n}$ 是 $X$ 的一个样本值，考虑到 $\hat{u}=u(\hat{\theta})$，且有 $\hat{\theta}=\theta(\hat{u})$，上式可写成：

$$
L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta(\hat{u})\right)=\max _{u \in \psi} L\left(x_{1}, x_{2}, \cdots, x_{n} ; \theta(u)\right)
$$ 
这就证明了 $\hat{u}=u(\hat{\theta})$ 是 $u(\theta)$ 的最大似然估计。

当总体分布中含有多个末知参数时，也具有上述性质。例如在正态分布中得到 $\sigma^{2}$ 的最大似然估计为

$$\hat{\sigma}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$$

函数 $u=u\left(\sigma^{2}\right)=\sqrt{\sigma^{2}}$ 有单值反函数 $\sigma^{2}=u^{2}(u \geqslant 0)$ , 根据上述性质, 得到标准差 $\sigma$ 的最大似然估计为

$$\hat{\sigma}=\sqrt{\hat{\sigma}^{2}}=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}$$

要注意的是，对数似然方程或对数似然方程组除了一些简单的情况外，往往没有有限函数形式的解，这就需要用数值方法求近似解。常用的算法是[牛顿迭代法](牛顿迭代法)或[梯度下降法](梯度下降法)等。




